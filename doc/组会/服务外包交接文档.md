# 服创交接文档

[TOC]

## 现状

- [ ] 考虑合理的实际场景应用细节！
- [ ] 解决自适应屏幕大小的分辨率问题（在屏幕尺寸和模型尺寸之间做一个衔接）
- [ ] 分割屏幕/编码图像（实现自动检测，当然对于少量的图像泄露可以直接手动标记）
- [ ] 解决如何局部解码（如何通过图像的局部来解释隐含信息。空间变换导致相对位置信息改变，让模型很难解码）
- [x] 图像分高/低频区域的不同损失权重，以生成感知差异性更小的编码图像（通过原图确定高低频区域，生成 amsk 并重写 MSE_Loss）
- [x] 显式地对遮挡进行鲁棒性训练（通过增加 随机遮挡RandomErasing 可以成功实现）
- [ ] 视频流服务部署
- [ ] 文档、PPT、视频、讲稿

## 快速使用

1. coco2014数据集下载：https://blog.csdn.net/u013249853/article/details/84924808
   - 数据集目录：放在 `HiddenWatermark/data/` 下，只需要是放置包含图片的文件夹即可
   - 一个文件夹对应一类数据集
2. 训练：直接运行 `train.py`
   - 在 `train_log` 保存日志与模型权重
   - 在 `tensorboard_log` 生成可视化训练过程
3. 编码：修改 `encoder.py` 对应参数并运行
4. 解码：保持与编码是同一组模型，运行 `decode`。

## 算法流程

### 项目总流程图

![服创大赛](服务外包交接文档/服创大赛.jpg)

### config文件

代码位置：`utils/config.py`

- 用一个py文件来定义了训练时所需要用到的参数，包括**损失权重、失真强度**的增长区间
- 使用 `get_cur_scales(cur_iter: int, cur_epoch: int, scale_name_list=None)` 来获取当前的可增长参数
- 在 `self.scale_list` 中写入对应的成员变量，该可增长参数才会生效
- `self.perspective_trans_max = 0.1`	表示透视变换的增长最大值为0.1
- `self.perspective_trans_grow = (0.3, 0.7)`	表示透视变换的增长区间为 (0.3, 0.7)
- `self.load_models = ['Encoder', 'Decoder', 'Discriminator']`	表示会在保存的权重文件中读取 'Encoder', 'Decoder', 'Discriminator' 这三个字段的模型参数
- 可以直接运行 `utils/config.py` 来测试观察可增长参数的变化，同时这些参数会在 tensorboard 中记录（但是并不直观）

关于学习率：

- StegaStamp 中：
  
  1. Adam。恒定学习率 0.0001
- 在以上为基础复现：

  1. warmup+cos 学习率调整。基础学习率 0.001，最高学习倍率1，最低学习倍率 0.1（在微调时最高学习倍率降低到 0.5，学习率区间压缩在0.0001-0.0005）
  2. 完成预热的 epoch 为 1，并且在第 0 个 epoch 之内完成各种递增权重的增长。这里说的递增权重就是用来控制**loss的计算**和**失真变化的强度**，这些参数也需要一个预热的过程。
  3. >  后续补充：由于学习率过高时，stn网络十分不稳定，所以**又改回了使用原论文中的恒定学习率 0.0001**

### 训练数据准备

StegaStamp 中：

1. 总体以 step（相当于一个iter而不是epoch）为单位来提供图片数据，每次取图片都是在数据集中随机采样，不要求遍历文件夹
2. 字符串信息为随机生成的 0 1 数组，作为输入的 label
3. 图片没有经过数据增强，直接使用原图
4. 总体的 step 为 140000 次，batch 为 4

在以上为基础复现：

1. 加入了随机裁剪、旋转扩充数据集（复现中**一共使用了两次数据增强**。这里的数据增强只是为了**扩充数据集**，对于后续训练过程并没有影响；第二次用于失真变换）
   - 注意，这里不能使用色彩抖动来扩充数据集，否则会影响detector的训练
2. coco2014train+val 数据集的 0.05 用于测试
3. 总体 10 epoch，每个 epoch 有 14640 iter，共 146400 iter，batch 为 8

代码位置：`utils/dataset.py`

- get_dataloader()：在设定随机种子的情况下，对 coco2014train+val 进行数据集分割，并设定数据增强、返回 dataloader 实例
- StegaDataset：定义数据集，并且**运行时生成随机二进制数组**作为 msg

### 网络结构

#### 编码器

在编码脚本中，字符串的嵌入流程：

1. 设定 0~7 个任意字符
2. 对字符串进行空格填充成 len=7
3. 对上述字符串生成 len=5 的 bch 校验码
4. 源字符串与校验码拼接成 len=12 的字符串
5. **字符串转二进制 len=96 ，转 list，补4个0位 len=100。**（这里开始与 train 相同）
6. **作为输入网络的参数，先单独进行全连接 [100] -> [7500]。**（这里开始进入 encoder 网络）
7. **reshape 成 [50\*50\*3]**
8. **8\*8 upsample 成 [400\*400\*3]**
9. **与原图 concat 成 [400\*400\*6]**

后续会进入一个 unet-like（削减了很多参数）的网络结构，最终输出一幅与输入图像大小相同的残差图。

代码位置：`models/stega_net.py`

尝试：

1. `models/stega_net.py` 的MyEncoder中，将上述第九步的cat，分成了两个独立的分支分别下采样img和msg，并在上采样时融合。
2. 结果暂时看不出好坏，参数量提升，但应该可以改善感知差异性。进一步确认那一种网络结构更好，**需要在后面空间变换的解决方案确定之后进行**。

#### 解码器

原论文实现

- 在进入普通的卷积网络之前，**先做了一步stn空间变换**，本质是通过卷积网络学习仿射变换的参数，将图像纠正（如去除白边、将图像移动放大到中心区域、复原位置信息）

- ![image-20211213174755095](服务外包交接文档/image-20211213174755095.png)
- STN 的优点：能够简单地将其插入任何现有的CNN，而且只需很少的修改。自动学习增强全局准确性的空间变换。
- STN **可以抵抗检测不稳定**（编码的位置信息出现失真）等因素，同时原论文说，加入透视变换之后，**图像生成方面也有比较大的进步**
- 缺点：1. 隐式学习，好像看见有几次学习并不到位；2. **对于梯度很敏感**，当loss、学习率、递增权重急剧变大时，模型会崩溃（0.693）。

代码位置：`models/stega_net.py`

- 需要待 decoder 部分稳定才可以开启 STN

尝试：

- `models/stega_net.py`的MyEncoder，将仿射变换替换为透视变换，并显式传出4个坐标进行损失计算。**目前还不确定最佳的损失函数！**
- 详细的实验结果在第二轮实验中提及

#### 判别器

- Discriminator 个人觉得没有什么用，因为我**将判别器的权重大小都限制在了[-0.25, 0.25]，并且当loss低于0.001时不加入损失计算，这样可以避免loss反向增长**。
- **这一部分的 Wasserstein loss 可能有误，先码一下，后续再看**。

### 失真变换

#### 影响程度较低的变换

1. 亮度 `brightness_trans`：(0.1, 0.2)epoch增长至 0.3（原论文此处1000iters完成增长，最终的变换强度相同）
2. 对比度 `contrast_trans`：(0.1, 0.2)epoch增长至 0.5（原论文此处1000iters完成增长，最终的变换强度相同）
3. 饱和度 `saturation_trans`：(0.1, 0.2)epoch增长至 1（原论文此处1000iters完成增长，最终的变换强度相同）
4. 色相 `hue_trans`：(0.1, 0.2)epoch增长至 0.1（原论文此处1000iters完成增长，最终的变换强度相同）
5. 模糊 `blur_trans`：(0.1, 0.2)epoch增长至 0.4（进行模糊的概率，原论文固定0.25概率添加模糊）
   - 原论文中的模糊包含了随机方向上的直线模糊，但我这里还只是实现了横向或者竖向的直线模糊
6. 成像噪声 `noise_trans`：(0.2, 0.3)epoch增长至0.02（原论文此处1000iters完成增长，最终的变换强度为0.02）

> 注：如果使用比stegastamp更复杂的encoder，则将花费更多时间来提升encoder的基础水平，因此要将上述参数的开始时间后移。

#### 影响程度较高的变换（注意，该部分频繁变更，这里展示的是之前的版本，新版本看**局部检测的失真-解码流程部分**）

1. 透视变换 `perspective_trans`：（第一个版本的透视变换，本质是对像素的压缩，不会产生位置变化，现在已经在`utils/distortion.py/make_trans()`中改变了逻辑）

   - 在复现中这个参数是指，在以原图的大小为基准，左上、左下、右上、右下 4 个区域中选择 4 个透视变换的目标坐标，对图像进行压缩、然后还原。参数的范围在 0~1
   - 原论文中的透视变换包含了位置信息的改变，故意在边缘引入了 0.1 的黑色部分，也就相当于中间这幅图直接返回，而不是像我这里完完全全变换回原位。我这里的复现，本质上是对像素的压缩再拉伸，**并不会改变位置的信息**，也正因如此我的变换因子才可以拉到0.7。
   - 当然，如果要想论文中实现的那样进行训练的话，一定要对 decoder 网络进行改造，比如**加上一层 STN，来学习增强模型的几何不变性**。
   - 原论文提到：“在解码器训练到一定水平之前，不能对图像进行损失计算。也不能对图像进行扰动失真，尤其是透视变换。“这句话应该是指位置信息的扰动对编码解码模式的影响是非常大的，所以要尽可能地缓慢加入透视变换。
   - 单纯看效果其实更像是模糊变换，且不同于高斯模糊，是被压缩强度越高的区域越模糊（右下角比左上更模糊）
   
2. 擦除遮挡 `erasing_trans`：

   - (0.3, 0.7)epoch增长至 0.7（后来又改成了0.5的最终强度，现在还不清楚需要采取多少的值）

   - 相当于把编码信息压缩在了未被擦除的地方，这个操作**依然不会改变位置信息**，这是与 RandomResizedCrop 的本质区别。
   - ![transformed_COCO_val2014_000000000042](服务外包交接文档/transformed_COCO_val2014_000000000042.jpg)

3. 观察角度 `angle_trans`：（本质是 RandomResizedCrop 的 ratio 参数，只是将其转换成角度）

   - 3和4是一起使用的，通过 RandomResizedCrop 实现。观察角度的变化主要是影响剪裁的长宽比（比如视角正对着图片的话，入射角0°，不会有任何像素被压缩；如果视角为60°的话，在横向上相当于原来两行的像素被压缩在了一行，如果不经过视角纠正（线性插值），那么输入进解码器的图片就是一个看上去长度被压成一半的图）。

   - <img src="进度对齐/image-20211016140102263.png" alt="image-20211016140102263" style="zoom: 80%;" />

   - 通过设置 RandomResizedCrop 的长宽比 ratio 来等效观察角度的变化。

   - > 后续补充：由于位置信息的改变，导致解码很难进行下去

4. 舍弃的图片区域 `cut_trans`：（本质是 RandomResizedCrop 的 scale 参数，只是将“剪裁出来的面积”转换成“1-舍弃的面积”）

   - 部分裁剪的参数主要是 裁剪区域:原图 的面积之比。

   - 部分裁剪消除了编码解码模式中可用的位置信息。这一点可以与随机遮挡来对比得知，**随机遮挡强度到0.7依然能够获得比较不错的效果，但是部分裁剪强度却只要0.2就能对编码解码模式造成比较大的影响**

   - ```python
     ratio = math.cos(scale["angle_trans"] / 180 * math.pi)
     if ratio != 0:
         img = transforms.RandomResizedCrop((width, height), scale=(1 - scale["cut_trans"], 1),
                                            ratio=(ratio, 1 / ratio))(img)
     ```

   - 不经过视角纠正（将不同视角下原图的四个角，透视变换回400\*400），那么想要放入解码器，那必定需要裁剪下一块矩形，然后再缩放回400\*400，然而此刻相对位置信息发生了改变

   - ![image-20211017130809510](服务外包交接文档/image-20211017130809510.png)

   - > 后续补充：详细的位置变化逻辑在第二轮优化中阐述

5. jpeg压缩 `jpeg_trans`：

   - 原论文在50个iter中压缩强度提高到75，复现是在(0.3, 0.7)epoch将强度提高到60

   - > 后续补充：
     >
     > - 现在的标准为(0.2, 0.3)增长至50
     > - jpeg压缩会对图像的感知差异性造成较大的影响

   - 感觉有些不可思议，因为直接上jpeg压缩的时候，模型直接0.693宕机了，分辩不了。不太懂为什么这里是50个iter完成0-75的强度增长。

   - 也有可能是因为相比原论文，复现中还加入了随机遮挡，这样的失真强度可能达到了模型本身的能力上限。

   - ![image](服务外包交接文档/diffjpeg.png)

### 损失函数

#### 当前所使用的损失：

- rgb_loss：rgb 通道上的 mse_loss
- hsv_loss：rgb 转 hsv 之后的 mse_loss（原论文没有）
- yuv_loss：rgb 转 yuv 之后的 mse_loss
- lpips_loss：一个基于深度学习的感知差异性损失（指导效果很强）
- msg_loss：二进制位解码结果与标签值的 binary_cross_entropy
- dis_loss：原图为 0 编码图为 1 的 binary_cross_entropy
- bit_acc：（不参与损失，仅计算准确率）二进制位级的准确率
- str_acc：（不参与损失，仅计算准确率）一个batch中有8张图，对应8条随机生成的二进制字符串，只有当整条字符串二进制位完全正确时，才会被认为是正确的（所以低一些没有太大问题）

```python
    # ------------------loss
    weight_mask = torch.abs(img - transforms_F.gaussian_blur(img, [7, 7], [10, 10]))
    weight_mask = torch.max(weight_mask) - weight_mask  # low weight in high frequency

    img_loss = torch.zeros(1).to(cfg.device)
    if scales["rgb_loss"] != 0:
        rgb_loss = mse_loss(encoded_img, img, mask=weight_mask)
        img_loss = img_loss + rgb_loss
    if scales["hsv_loss"] != 0:
        hsv_loss = mse_loss(rgb_to_hsv(encoded_img), rgb_to_hsv(img), mask=weight_mask)
        img_loss = img_loss + hsv_loss
    if scales["yuv_loss"] != 0:
        yuv_loss = mse_loss(rgb_to_yuv(encoded_img), rgb_to_yuv(img), mask=weight_mask)
        img_loss = img_loss + yuv_loss
    if scales["lpips_loss"] != 0:
        lpips_loss = lpips(img, encoded_img).mean()
        img_loss = img_loss + lpips_loss

    position_loss = nn_F.l1_loss(startpoints_predict, startpoints)
    msg_loss = nn_F.binary_cross_entropy(msg_pred, msg)  # size(B, 100)的msg，二分类0和1
    dis_loss = nn_F.binary_cross_entropy(dis_pred, dis_label)  # 包含原图和编码图，cat成size(2B, )预测是否为编码图

    loss = img_loss + msg_loss
    if torch.ge(dis_loss, 0.001):  # 当鉴别器在一定损失之内时，不对其进行优化
        loss = loss + dis_loss
        
def mse_loss(pre, tar, mask):
    return torch.mean((pre - tar) ** 2 * mask)
```

#### 参数递增问题

在权重的递增过程中，失真变换权重以及损失的权重都是缓慢变化的，所以在增加的一段时间内，无法对模型进行一个统一的评判。比如说：

1. 在失真变换还没有拉满的时候，模型可能已经适应了当前的变换水准，所以在解码率acc上有不错的表现
2. 在失真变换达到了我们预设的标准之后，模型可能在解码上acc有一定下降，但实际上这时候的模型能力不一定比之前差
3. 在前者的基础上继续增加失真的强度，就可能超出模型理论上限的抗压标准，这时模型为了适应高强度的失真变换，就会导致生成图像的质量下降等等。

- 目前采取的办法是把权重的变化都放在第一个epoch中增加，后续就是一个定值，就有了一个固定的判断标准

- 但是这有一个问题就是，在实验之前，我们不知道多少的抗压强度是模型的能力上限，**因此无法准确地确定该变化参数的max**。

- > 后续补充：
  >
  > - 参数增长的先后关系也有可能影响到最终效果
  >
  > - 关于"一张图像上的高低频区域设定不同的损失权重"在第二轮优化部分阐述

#### 关于权重mask

对普通mse作进一步改进，这是我们的一处相较于原论文的改进方向：

- **仅通过原图来确定高低频区域**，对不同的区域施加不同程度的 loss

  - 我们希望模型在高频部分编码，其实就是等价于**高频部分的编码图和原图的差异是可以容忍的**；
  - 我们不希望模型在低频部分编码，其实就是等价于**低频部分的编码图和原图的差异要最小化**。

- > 可以对任意一张图片做自适应的**位置损失权重**

- 需要和其他损失配合食用，这是和 yuv_loss 的例子

  ```python
  def mse_loss(pre, tar, mask):
      return torch.mean((pre - tar) ** 2 * mask)
  
  yuv_loss = mse_loss(rgb_to_yuv(encoded_img), rgb_to_yuv(img), mask=weight_mask)
  ```

下面是 原图、原图与原图高斯模糊后的残差图（越亮代表频率越高）、将残差图的高低频数值对调获得的 weight_mask（越亮越白表示施加的 loss 权重越大，越黑或者彩色表示某些通道的 loss 越低）

```python
blur_loss_trans = transforms.GaussianBlur((7, 7), sigma=10)
mask = torch.abs(img0 - blur_loss_trans(img))
mask = torch.max(mask) - mask  # 在范围内翻转高低频区域的数值，作为最终的损失权重
print(torch.max(mask))  # 0.6370
print(torch.min(mask))  # 0
```

![image-20211114120046322](服务外包交接文档/image-20211114120046322-1638535957233.png)![image-20211114115411408](服务外包交接文档/image-20211114115411408-1638535957236.png)![image-20211114115802126](服务外包交接文档/image-20211114115802126-1638535957236.png)

> 补充：可以尝试使用其他方法（如傅里叶）来更好地获取高低频信息，只要是可导

### 局部检测的 失真-解码 流程（不需要在意这一部分，仅是当前的代码逻辑，服创用不到）

1. 通过编码器获得编码图：

   - ![image-20211114140801149](服务外包交接文档/image-20211114140801149-1638513327200.png)

2. 在训练的权重递增过程中，将随机获得三组参数，分别对应红绿蓝

   - 红色为我们**最终会裁取的感兴趣区域**（为了方便，始终保持在了绿色框内进行随机采样，由 RandomPerspective 相同的机制获取随机参数，即 `perspective`）
   - 绿色是**第一次空间变换会达到的位置**（由 RandomResizedCrop 相同的机制获取随机参数，即 `angle`入射角、`cut`丢弃面积）
   - 蓝色是**编码器输出以及解码器输入的大小（400，400）**
   - ![image-20211114151158024](服务外包交接文档/image-20211114151158024-1638513327200.png)

3. 进行**第一次透视变换，模拟相机拍摄角度的空间变换**。

   - 红色（我们即将裁剪的矩形区域）到达了原先绿色框框的位置。
   - ![image-20211114151347664](服务外包交接文档/image-20211114151347664-1638513327200.png)

4. 对整张图进行失真扰乱（**亮度、对比度、饱和度、色相、模糊、jpeg压缩、随机噪声**）

   - ![image-20211114151402046](服务外包交接文档/image-20211114151402046-1638513327200.png)

5. **第二次透视变换**，将我们框选的部分缩放为（400，400）即解码器网络的输入尺寸（只需要关注最后我们裁取的感兴趣区域，其他没有被红框框住的部分被丢弃）
   - ![image-20211114151408808](服务外包交接文档/image-20211114151408808-1638513327200.png)

6. **第三次透视变换**，是在模型内部进行，模型将要通过第五张图，**预测红色框的四个坐标**（该预测结果将会拿出来与已知的红框坐标参数做 L1 损失），获得透视变换矩阵，将解码器的输入变换至原编码图的对应位置
   - 理想情况下，我们将预测该输入对应原编码图的对应区域如下（即只有图2的红框部分）
     - 等价于随机擦除感兴趣区域以外的周边区域，如果位置变化不大，将**与随机擦除的等价**

   - ![image-20211114151443374](服务外包交接文档/image-20211114151443374-1638513327200.png)

7. 最后附原编码图对比

   - ![image-20211114140801149](服务外包交接文档/image-20211114140801149-1638513327200.png)

## 代码简述

### train.py

运行 `train.py` 即可进行模型训练。

1. 首先会读取 `utils/config.py` 的设置。暂时只支持在 py 文件中修改设置，还不是很方便。
2. 实例化 tensorboard、logger
3. 通过 `utils/dataset.py` 获得 dataloader
4. 实例化模型。该部分的变动很有可能对后面代码造成影响，因为不同的模型可能会在前馈过程返回不同的参数（比如 MyDecoder 将显式地传回预测的透视变换参数进行损失计算）
5. 定义优化器和学习率策略。暂时只使用恒定学习率，使用warmup会导致带有空间变换的decoder模型崩溃
6. 以epoch为单位进行训练

### utils/train_utils.py: train_one_epoch

1. 先进入该文件夹下的 `train_one_epoch()`
2. 通过设置对象的 `cfg.get_cur_scales` 获取当前epoch、当前iter的动态可增长参数
3. 进行前馈和损失计算
4. 反向传播。此时如果decoder传回position_loss，则独立于总loss之外先累计一次梯度，若不传回，则直接注释掉
   - 同时，若decoder不传回position_loss，要在 `make_null_metric_dict()` 的METRIC_LIST中、`process_forward`的metric_result 中删去对应的position_loss字段
5. 累加一个iter的result，每隔`cfg.log_interval`打印一次输出
6. 写了两行代码保存encoded_img和空间变幻后的图，以便在训练过程中随时观察变化
7. 限制dis模型的参数大小，否则loss会反向升高

### utils/train_utils.py: process_forward（需要改动）

<img src="服务外包交接文档/image-20211203203249477.png" alt="image-20211203203249477" style="zoom:150%;" />

这里的逻辑即上图所示（可以修改）

需要注意的是：

1. 暂时通过一个高斯模糊及一系列逻辑来获得一个损失权重的掩码，其中lpips_loss无法使用该mask
3. 当鉴别器在一定损失之内时，不对其进行优化
3. 非空间的失真扰乱需要对背景和前景一起做，但是第一轮的空间变换只对编码图进行

### utils/distortion.py: make_trans（需要改动）

其中所有的失真变换写在这里

> 注意：失真变换的过程顺序要符合实际的应用场景
>
> 注意：这里改动频繁，并且需要进行进一步的探讨

1. 进行第一次空间变换。这里对应了 失真-解码 流程的第三张图，是为了模拟相机的拍摄角度变换。
2. 进行一系列非空间变换，包括成像噪声、色彩抖动、运动模糊。
3. 缩放回解码器大小的空间变换。代表了实际场景中已经选取了目标区域，放入decoder中解码
4. 该方法返回
   - 经过一系列失真并且最终大小为400^2的`transformed_img`
   - 经过理想的透视变换回到原位的`no_stretched_img`。该图像仅写入tensorboard，而不进行任何计算
   - 转化为tensor的透视变换参数。该参数将作为label，与stn预测出的参数数值进行损失计算

## 资料

http://www.fwwb.org.cn/topic/show/199c8731-ece6-45df-bfb0-8595ddd2980c

https://blog.csdn.net/MajorDong100/article/details/78470176/

https://zhuanlan.zhihu.com/p/377185953

https://blog.csdn.net/sansheng0208/article/details/106520795/